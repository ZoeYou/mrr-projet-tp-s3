---
title: "Untitled"
author: "You ZUO, Jingzhuo HUI"
date: "2019/9/16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
tab <- as.data.frame(as.matrix(read.table("Files/TP1/immo.txt",header = T,sep = ";")))
```

```{r}
head(tab)
names(tab)
tab[,1]
tab$surface
```
```{r}
modreg <- lm(prix~., data=tab)
print(modreg)
summary(modreg)
attributes(modreg)
coef(modreg)
modreg$residuals
plot(modreg)
```
```{r}
hat_y <- fitted(modreg)
ipsilon <- tab$prix - hat_y
```

## ii
### a)
```{r}
icedata <- as.data.frame(read.table("Files/TP1/Icecreamdata.txt",sep = ";",header = T))
dim(icedata)
```

$$cons=\beta_0+\beta_1\times income+\beta_2\times price+\beta_3\times temp$$
```{r}
mod.ice <- lm(formula = cons~.,data = icedata)
mod.ice
summary(mod.ice)
```
###b) Estimated coefficients
```{r}
library(matlib)
X <- as.matrix(cbind(rep(1,nrow(icedata)), icedata[,-1]))
Y <- as.matrix(icedata[,1])
beta <- inv(t(X)%*%X)%*%t(X)%*%Y
beta
```

our statistic is 
$$\frac{\hat{\beta}_j}{\sqrt{\hat{\sigma}^2S_{j,j}}}$$
with $S_{j,j}$ jth term of the diagonal of $(X^TX)^{-1}$ et $\hat{\sigma}^2=\frac{||\hat{\epsilon}||^2}{n-p}$
the condition for us to reject our hypothesis $H_0)$ is that 
$$\frac{|\hat{\beta}_j|}{\sqrt{\hat{\sigma}^2S_{j,j}}}>t_{n-p}(1-\frac{\alpha}{2})$$
or
$$p-value<\alpha$$
the value of the statistics 
```{r}
d <- inv(t(X)%*%X)
epsilon.ice <- icedata$cons  - as.vector(fitted(mod.ice))
n <- nrow(icedata)
p <- ncol(icedata[,-1])
sigma2_est <- crossprod(epsilon.ice)/(n-p)
stat <- sapply(1:length(beta), function(i){
  beta[i]/(sqrt(sigma2_est*d[i,i]))
})
stat

alpha <- 0.05
qt(p = 1-alpha/2, df = n-p)
```

the associated-pvalue
the p-value or probability value is, for a given statistical model, the probability that, when the null hypothesis is true, the statistical summary (such as the sample mean difference between two groups) would be equal to, or more extreme than, the actual observed results.
```{r}
pt(q = stat, df = n-p, lower.tail = F) <= alpha/2
```
From the results below we can see that the second and the fourth variable "income" and "temps" are very significant to our variable target.
```{r}
confint(object = mod.ice, level = 0.95)
confint(object = mod.ice, level = 0.99)
```
```{r}
library(ggplot2)
Y_pre <- fitted(object = mod.ice)
ggplot(data = data.frame(Y,Y_pre),aes(Y,Y_pre)) +
  geom_point()

predict(object = mod.ice, level = "confidence")
Y_pre
```

RMSE
$$\sqrt{\sum_{i=1}^n{(Y_i-\hat{Y_i}})^2}=\sqrt{\hat{\sigma}^2}$$
```{r}
library(hydroGOF)
rmse(as.vector(Y),as.vector(Y_pre))
```
we are going to calculate the non-biased variance of $\epsilon_1,\epsilon_2,...,\epsilon_n$, which the equation is 
$$\mathcal{D}_{non\ biased}[\epsilon_1,\epsilon_2,...,\epsilon_n]=\frac{1}{n-1}\sum_{i=1}^n(\epsilon_i-\bar{\epsilon})^2$$
```{r}
var(epsilon.ice)
plot(mod.ice, which = 1:2)
```


From the qq-plot we can see that those predicted values and the obseved targets only fit in the middle part, and at the same time it has a very obvious symmetry tendency for the head and tail, which infers that we may have ignored some rules between our values.

```{r}
shapiro.test(epsilon.ice)
```
Here we have $p-value=0.1195\geq0.1$, so we can not reject $H_0$ for which the sequence of residuals follow a rule of normality.

### e)
```{r}
newdt <- data.frame(income = 50, price = 0.28, temp = 50, cons = NA)
predict(object = mod.ice, newdata = newdt, level = 0.95, interval = "confidence")

as.matrix(cbind(1,newdt[,-4]))%*%beta
```

### f)
```{r}
ind <- sample(2, nrow(icedata),replace = TRUE,prob = c(0.75, 0.25))
TabTrain <- icedata[ind==1,]
TabTest <- icedata[ind==2,]

mod.train <- lm(data = TabTrain, formula = cons~.)
rmse(fitted(mod.train), TabTrain$cons)
```

```{r}
test.pre <- predict.lm(object = mod.train, newdata = TabTest)
rmse(test.pre,TabTest$cons)
```
```{r}
rmse_2 <- data.frame(rmse=NA,source=NA)
for (i in 1:10) {
  ind <- sample(2, nrow(icedata),replace = TRUE,prob = c(0.75, 0.25))
  TabTrain <- icedata[ind==1,]
  TabTest <- icedata[ind==2,]
  
  mod.train <- lm(data = TabTrain, formula = cons~.)
  rmse_2[2*i-1,] <- cbind(rmse(fitted(mod.train), TabTrain$cons),"Traing")
  
  test.pre <- predict.lm(object = mod.train, newdata = TabTest)
  rmse_2[2*i,] <- cbind(rmse(test.pre,TabTest$cons), "Testing")
}

rmse_2$rmse <- as.double(rmse_2$rmse)

ggplot(data = rmse_2, aes(source, rmse, fill = source, colour = source)) +
  geom_boxplot(alpha=0.25, outlier.alpha=0) +
  geom_jitter(fill="black", size = 0.1) +
  stat_summary(fun.y=mean, colour="white", geom="point", shape=18, size=1) 
```

## \uppercase\expandafter{\romannumeral3}
### a)
```{r}
iceData <- read.table("Files/TP1/Icecreamdata.txt", sep = ";", header = T)
iceData <- cbind(iceData, var5 = rnorm(n = nrow(iceData)))

mod.ice <- lm(formula = cons~., data = iceData)
mod.ice
summary(mod.ice)
```

```{r}
library(hydroGOF)
Y <- iceData$cons
Y_pre <- fitted(object = mod.ice)
sqrt(x = sum((Y-Y_pre)^2)/length(Y))
rmse(Y,Y_pre)
```

### b)
```{r}
iceData <- iceData[,-5]
res <- data.frame(RMSE = NA, R2 = NA)
mods.ice <- mod.ice

for (k in 1:20) {
  iceData <- cbind(iceData, rnorm(nrow(iceData)))
  colnames(iceData)[ncol(iceData)] <- paste("var", k, sep = "")
  mod.ice <- lm(formula = cons~., data = iceData)
  Y <- iceData$cons
  Y_pre <- fitted(object = mod.ice)
  res[k,] <- c(rmse(Y,Y_pre),summary(mod.ice)["r.squared"][[1]])
  print(mod.ice)
  
}


```

```{r}
library(ggplot2)
ggplot(data = res, aes(x = 1:nrow(res), RMSE)) +
  geom_point() +
  xlab(label = "k") +
  ylab(label = "RMSE(k)")

ggplot(data = res, aes(x = 1:nrow(res), R2)) +
  geom_point() +
  xlab(label = "k") +
  ylab(label = "R^2(k)")
  
```
